from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.tools import BaseTool
from tutor_helper.common.llms import LlmLoader
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema import Generation, LLMResult


import json
import logging
logger = logging.getLogger(__name__)


class SearchTerm(BaseTool):
    name:str = "SearchTerm"

    input_schemas:list = [
        ResponseSchema(
            name="description", description="input sentence to generate the search term"
        )
    ]
    input_parser: StructuredOutputParser = StructuredOutputParser.from_response_schemas(input_schemas)
    input_format: str = input_parser.get_format_instructions()

    output_schemas:list = [
        ResponseSchema(name="search_term", description="generated search term")
    ]
    output_parser: StructuredOutputParser = StructuredOutputParser.from_response_schemas(output_schemas)
    output_format: str = output_parser.get_format_instructions()

    description:str = (
        "Generates the search term based on a input, useful for DB searches."
        f"\nInput Format: {input_format}. "
        f"\nOutput format: {output_format}"
    )

    prompt:str = """
Create a search term based on the description provided. The search term should be in {language} and constructed in such a way that it is optimized for use with Web Search keyword and vector search functionalities to find relevant documents or troubleshooting guides. The search term must capture the essence of the issue described, using key technical terms and context cues to ensure accurate and helpful search results. Respond with the search term only!
INPUT: ```{description}```
"""
    def _run(self, query: str) -> str:
        # Parsing input query/product json
        request = self.input_parser.parse(query)
        model_name = LlmLoader.DEPLOYMENT_35_TURBO

        # To filter responses generated by specific LLM model (e.g. gpt-4)
        params = {}
        params["llm"] = model_name
        params["tm_tool"] = self.name
        prompts = [request["description"]]

        problem_description = prompts[0]            
        llm = LlmLoader.create_chat_llm(
            model=model_name,
            temperature=0.4,
            top_p=0.4,
            verbose=True,
        )

        search_query_chain = LLMChain(
            verbose=False,
            llm=llm,
            prompt=PromptTemplate.from_template(self.prompt),
        )
        language = "english"
        request.update({"language": language})
        search_term = search_query_chain(request)
        logger.info(f"Search Term: {search_term}")

        return '```json{"search_term":' + json.dumps(search_term) + "}```"
    
    def _arun(self, query: str) -> str:
        logger.info("arun and QUIT")
        quit()

    def from_description(self, description: str):
        """Input and output are in raw strings"""
        response = self._run(
            '```json{"description":' + json.dumps(description) + "}```"
        )
        response_json = self.output_parser.parse(response)
        return response_json["search_term"]
